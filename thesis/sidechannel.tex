\chapter{Side Channel Attacks}

Shared resources of the processor can leak information about the tasks
being performed in it as shown in \Figref{fig:dls}.
This leaked information may be extracted by an attacker using
various means. The attacker will try to use some form of measurement like
cache hit/miss, time of execution, power consumption, EMI spikes to determine
what part of code is running or what data is being processed
\Citeref{side-channel}. These kind of attacks have been proven to be effective
on cryptography algorithms.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.7\textwidth]{figures/side_channel}
    \caption[Data leakage sources]{A number of side channels
    which are capable of leaking data.}
    \label{fig:dls}
\end{figure}

\section{Data dependent execution in encryption algorithms}

Encryption standards like RSA, ECDSA, AES have been implemented in programs in
a way which causes certain branches and memory access patterns to be dependent
on the secret key. By using side channels to analyse which branch was taken or
detect which memory address was loaded, it is possible to decode the secret
key. For example, \Lstref{lst:kdrsa} shows the key-dependent branch of fast
exponentiation part of RSA. Fast exponentiation works by repeated squaring and
multiplying when bit of the exponent is 1. In RSA, the secret key is used as
the exponent hence we get a bit-by-bit difference in executed code. When
analysing power trace or measuring timing of the execution of this part of
code, we can infer that higher power consumption and larger execution time
occur when key bit is 1. This proves that there is information being leaked
bit by bit.

\begin{lstlisting}[caption={Key-dependent branch of fast
exponentiation used in RSA},label={lst:kdrsa},language={C}]
while(key > 0) {
    e = key % 2;

    Square();
    Reduce();
    if(e == 1) {
        Multiply();
        Reduce();
    }

    key >>= 1;
}
\end{lstlisting}

In algorithms like AES and DES, P-box and S-box are used for fast permutation
and substitution. They essentially store a mapped permutation or substitution
for each key value. This means that during execution, AES algorithm will
access various different blocks from S-box and P-box memory region depending
on the key which is being used. If we can trace these memory accesses in some
way, we can infer the secret key. Memory buses leak data about the address via
EMI channel, and by analysing that we can get a trace of the memory access
pattern. A better and more effective way of obtaining memory access patterns
is by analysing the cache.

\section{Cache side channel}

All the threads running in a single core use the same L1 caches inside that
core. Processes running on two different cores in a multi-core processor share
the Last level cache. The data access patterns of a process leaves behind
fingerprints in the cache. Because of set-associativity, if we can determine
which cache line is being accessed by the process, we can determine the actual
address which was accessed.

This is done without ever having to read the actual cache line, by causing
contention on that cache line by an attacker process
\Citeref{cache-side-channel}. When the attacker and victim are both trying to
use the same cache line, the attacker will get noticeable difference in
execution time due to cache misses. There are various ways in which a cache
side channel can be created.

\section{Prime+Probe}

The steps followed by Prime+Probe attack are as follows:

\begin{enumerate}
\item Attacker primes the cache line by loading his own data which .
\item Victim process runs and accesses memory mapped to same cache line, hence
    evicting attacker's data.
\item Attacker probes the cache line by reloading the same data, and looking
    for a cache hit/miss.
\end{enumerate}

\begin{figure}[h]
\includegraphics[width=0.9\textwidth]{figures/prime_probe}
\caption[Prime Probe attack]{Example of a prime-probe attack on single L1 cache set.
    Miss in the PROBE step can be noticed by increased execution time}
\label{fig:pp}
\end{figure}

A miss in the probe step results in increased code execution time for the attacker,
which it can easily measure by reading the Time Step Counter present in many
modern cores. As shown in \Figref{fig:pp}, attacker has to prime the entire
cache set (all ways) for a successful attack. For analysing the victim's every
memory access, attacker needs to prime the entire cache. This priming step
leads to a lot of cache misses and can be tracked by event counters and
trigger security exceptions when the cache misses reach an alarming amount.
Moreover, in cases where attacker and victim are not colocated on the same
core, such an attack would have to use a lower level of shared cache like LLC.
The probing step requires LLCs to be fully inclusive else the victim will not
evict attacker's data from the LLC and not lead to the required cache miss.

\section{Flush+Reload}

Flush+Reload is a side channel attack on caches which relies on the
\texttt{clflush} instruction present in X86 ISA (and similar variants in other
ISAs). Flush+Reload is able to work at a finer granularity than Prime+Probe.
It is also able to successfully mount cross-core attacks via the LLC.

\begin{enumerate}
\item Attacker flushes a cache line using \texttt{clflush}.
\item Victim process runs and accesses memory hence loading the flushed block
    into cache.
\item Attacker reloads the same data, looking for a cache hit/miss.
\end{enumerate}

\begin{figure}[h]
\includegraphics[width=0.9\textwidth]{figures/flush_reload}
\caption[Flush Reload attack]{Example of a flush-reload attack on single L1
    cache set. Hit in the RELOAD step can be noticed by decreased execution time}
\label{fig:fr}
\end{figure}

As seen in \Figref{fig:fr}, the granularity of Flush+Reload is at cache line
level rather than cache set level. This happens because the attacker tries to
access the same data as the victim, instead of creating contention with other
data mapping to the same cache set. Accessing same data is possible because
majority of encryption algorithms are provided as system-wide shared
libraries. Both the code and data regions of these libraries can be accessed
by all processes. As opposed to Prime+Probe, this makes Flush+Reload a very
practical and efficient attack. Flush+Reload is able to achieve greater
granularity and accuracy due to it scanning for Cache Hit instead of Cache Miss.

\begin{figure}[h]
\centering
\includegraphics[width=0.5\textwidth]{figures/flush_reload_crossvm}
\caption[Cross-VM Flush+Reload]{Flush+Reload via the LLC enables mounting Cross-VM attacks. This exploit is extremely significant in cloud computing environments}
\label{fig:crossvm}
\end{figure}

Flush+Reload is also effective on LLCs because inclusivity will not affect
\texttt{clflush} behaviour, hence attacker will get an LLC hit when the victim
process accessed data. This opens up possibility of mounting a Cross-VM attack
\Citeref{cross_vm} like shown in \Figref{fig:crossvm}

\section{Reverse engineering cache parameters}

This implementation of reverse engineering cache parameters is based on
\Citeref{gpu_reverse}. In that paper, Wong et al show how using a stride
access pattern over an array to trigger a predictable number of cache-misses.
By measuring latency of stride access, we can get an idea of the number of cache misses.

For a given array size, we need to feed in the stride pattern into the array
i.e. \texttt{array[i]} should contain location of \texttt{array[i+STRIDE]}. We can create such an
array pattern offline before starting timing measurements. In this way, we can
do a linked-list like traversal of the array without needing to calculate next
stride location online. \Lstref{lst:offline} shows how one can create the
array with a stride pattern.

\begin{lstlisting}[label={lst:offline},caption={Offline
formation of array with stride access pattern},language={C}]
size_t* array; // malloced beforehand
size_t t;

for (int i=0; i<array_size; i++) {
    t = i + STRIDE:
    if(t >= array_size) t %= STRIDE;
    array[i] = (size_t)array + sizeof(size_t)*t;
}
\end{lstlisting}

For measuring timing of the array access, \texttt{rdtsc} instruction is used
to get a reading of the Time Step Counter before and after accessing the
array. The difference is plot versus array size. \Lstref{lst:array_access}
shows how to traverse the array using the stride access data stored in it. The
\texttt{next\_ptr} variable stores pointer to next element to access. It is
dereferenced and the loaded data is again stored into \texttt{next\_ptr} for
the next iteration.

\begin{lstlisting}[label={lst:array_access},caption={Timing measurement of stride access over the entire array},language={C}]
long start = __rdtsc();
size_t* next_ptr = &array[0];
for(int i=0; i<MAX_ITERS; i++) {
    next_ptr = *((size_t**)next_ptr);
}
long time = __rdtsc() - start;
\end{lstlisting}

\Figref{fig:gem5_cache} shows a plot of latency vs array size. The latency
plot stays constant initially until an array size which fills up the whole
cache. Once that happens, some lines in the cache start getting evicted and we
see a steep rise in latency. After any rise, the latency stays constant for
the line size of the cache. This is obvious because any access in the same
cache line will incur same total latency as there will only be one cache miss.
This latency rise occurs once for each cache set, because as long as there are
new cache sets to evict, there will be misses. The latency increase stops when
one whole way of the cache is replaced once by the array access. The starting
point of latency increase gives us cache size. The step width gives us line
size. Number of steps gives number of sets, but that is hard to clearly
determine when noise is present in measurements. Thus we determine way size by
looking at the point where the latency plot flattens out again. Then
$\mathrm{# sets} = \frac{\mathrm{way size}}{\mathrm{line size}}$.

\subsection{Experimental setup}

For all cases, stride of 64B was used.

One set of simulations was done using gem5 simple CPU and configurable cache
sizes. This was done for testing out the algorithm. \Figref{fig:gem5_cache}
was plot for L1 data cache of 1KB size, 2-way, 64B line size.
\Figref{fig:gem5_cache2} was plot for L1 data cache of 16KB size, 4-way, 64B
line size.

The same algorithm was run on Intel Skylake i5-6500 processor with L1 cache of
32KB size, 8-way, 64B line size. The latency plot is shown in
\Figref{fig:x86_cache}. As is seen, there is some amount of noise due to
Out-of-Order processing and other programs interfering with the execution of
the latency measurements. Despite the noise, we can clearly make out the
steps, beginning of the latency increase, and way size. This gives us every
parameter required for the cache.

\begin{figure}[h]
\centering
\includegraphics[width=\textwidth]{figures/reverse_eng_1kb}
\caption[1KB cache stride access]{Latency vs. Array size plot
    for a 1kB 2-way cache with 64B cache line.}
\label{fig:gem5_cache}
\end{figure}

\begin{figure}[h]
\centering
\includegraphics[width=\textwidth]{figures/reverse_eng_16kb}
\caption[1KB cache stride access]{Latency vs. Array size plot
    for a 16kB 4-way cache with 64B cache line.}
\label{fig:gem5_cache2}
\end{figure}

\begin{figure}[h]
\centering
\includegraphics[width=\textwidth]{figures/reverse_eng_32kb}
\caption[1KB cache stride access]{Latency vs. Array size plot
    for a 32kB 8-way cache with 64B cache line. Run on Intel Skylake i5-6500}
\label{fig:x86_cache}
\end{figure}

